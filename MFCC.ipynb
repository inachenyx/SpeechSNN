{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQWvsp2u5Sma2X7LXD2Yvj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inachenyx/SpeechSNN/blob/main/MFCC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing - Feature scaling (MFCC feature extraction)\n",
        "#### sklearn.preprocessing"
      ],
      "metadata": {
        "id": "hC-0IleHn6JY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8nG3WVklwG4"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "def extract_features(audio,rate):\n",
        "   mfcc_feature = mfcc.mfcc(audio,rate, winlen=0.020,preemph=0.95,numcep=20,nfft=1024,ceplifter=15,highfreq=6000,nfilt=55,appendEnergy=False)\n",
        "   mfcc_feature = preprocessing.scale(mfcc_feature)\n",
        "   delta = calculate_delta(mfcc_feature)\n",
        "   combined = np.hstack((mfcc_feature,delta))\n",
        "   return combined"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing - Stereo to mono format\n",
        "#### pydub"
      ],
      "metadata": {
        "id": "kbNzjM54oEfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "mysound = AudioSegment.from_wav(\"stereo_infile.wav\")\n",
        "# set mono channel\n",
        "mysound = mysound.set_channels(1)\n",
        "# save the result\n",
        "mysound.export(\"mono_outfile.wav\", format=\"wav\")"
      ],
      "metadata": {
        "id": "L9SAKUd_n3xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VAD Voice Activity Detection\n",
        "#### PyTorch"
      ],
      "metadata": {
        "id": "PyQOOimWvFR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# loading vad model and tools to work with audio\n",
        "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False)\n",
        "\n",
        "(get_speech_ts_adaptive, save_audio, read_audio, VADIterator, collect_chunks) = utils\n",
        "\n",
        "audio = read_audio('raw_voice.wav')\n",
        "\n",
        "\n",
        "# get time chunks with voice\n",
        "speech_timestamps = get_speech_ts_adaptive(wav, model)\n",
        "\n",
        "\n",
        "# gather the chunks and save them to a file\n",
        "save_audio('only_speech.wav',\n",
        "         collect_chunks(speech_timestamps, audio))"
      ],
      "metadata": {
        "id": "ONPS-XFYxKhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Noise Reduction\n",
        "### noisereduce, SciPy"
      ],
      "metadata": {
        "id": "vyicKsi32Gjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import noisereduce as nr\n",
        "from scipy.io import wavfile\n",
        "\n",
        "# load data\n",
        "rate, data = wavfile.read(\"voice_with_noise.wav\")\n",
        "\n",
        "# perform noise reduction\n",
        "reduced_noise = nr.reduce_noise(y=data, sr=rate)"
      ],
      "metadata": {
        "id": "ygkCoW1b2DNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction\n",
        "#### For example, Delta MFCC combined with regular MFCC\n",
        "#### numpy, scikit-learn, python_speech_features"
      ],
      "metadata": {
        "id": "Crlu9kSA57GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from python_speech_features import mfcc, delta\n",
        "\n",
        "def extract_features(audio,rate):\n",
        "  \"\"\"extract 20 dim mfcc features from audio file, perform CMS and combine\n",
        "  delta to make 40 dim feature vector\"\"\"\n",
        "\n",
        "  mfcc_feature = mfcc.mfcc(audio, rate, winlen=0.020,preemph=0.95,numcep=20,nfft=1024,ceplifter=15,highfreq=6000,nfilt=55,appendEnergy=False)\n",
        "\n",
        "  # feature scaling\n",
        "  mfcc_feature = preprocessing.scale(mfcc_feature)\n",
        "  delta_feature = delta(mfcc_feature, 2) # calculating delta\n",
        "  # stacking delta features with common features\n",
        "  combined_features = np.hstack((mfcc_feature, delta_feature))\n",
        "  return combined_features"
      ],
      "metadata": {
        "id": "UAs7CRL16Re6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with Gaussian Mixture Model\n",
        "#### sklearn.mixture"
      ],
      "metadata": {
        "id": "Z2lhNxkb_HBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_rate, data = read('denoised_vad_voice.wav')\n",
        "\n",
        "# extract 40 dimensional MFCC & delta MFCC features\n",
        "features = extract_features(audio, sr)\n",
        "\n",
        "gmm = GMM(n_components=16,max_iter=200,covariance_type='diag',n_init=1, init_params='random')\n",
        "gmm.fit(features)  # gmm training"
      ],
      "metadata": {
        "id": "7dtrMCnT_Otw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GMM+UBM Universal Background Model"
      ],
      "metadata": {
        "id": "ra5DqHTUFbqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ubm = bob.kaldi.ubm_train(features, r'ubm_vad.h5', num_threads=4, num_gauss=2048, num_iters=100)\n",
        "\n",
        "\n",
        "# training every gmm using ubm\n",
        "user_model = bob.kaldi.ubm_enroll(features, ubm)\n",
        "\n",
        "\n",
        "# scoring using ubm and a specified gmm\n",
        "bob.kaldi.gmm_score(features, user_model, ubm)"
      ],
      "metadata": {
        "id": "uUHvVwY7FaJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Learning Models"
      ],
      "metadata": {
        "id": "Q_lQvlYGIAp8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "def deep_model(model: str = 'speakernet', quantized: bool = False, **kwargs):\n",
        "  \"\"\"\n",
        "  Load Speaker2Vec model.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  model : str, optional (default='speakernet')\n",
        "      Model architecture supported. Allowed values:\n",
        "\n",
        "      * ``'vggvox-v1'`` - VGGVox V1, embedding size 1024\n",
        "      * ``'vggvox-v2'`` - VGGVox V2, embedding size 512\n",
        "      * ``'deep-speaker'`` - Deep Speaker, embedding size 512\n",
        "      * ``'speakernet'`` - SpeakerNet, embedding size 7205\n",
        "\n",
        "  quantized : bool, optional (default=False)\n",
        "      if True, will load 8-bit quantized model.\n",
        "      The quantized model isnâ€™t necessarily faster, it totally depends on the machine.\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  result : malaya_speech.supervised.classification.load function\n",
        "  \"\"\"\n",
        "\n",
        "model = malaya_speech.speaker_vector.deep_model('speakernet')\n",
        "\n",
        "from glob import glob\n",
        "\n",
        "speakers = ['voice_0.wav', 'vocie_1.wav', 'voice_2.wav']\n",
        "\n",
        "# pipeline\n",
        "def load_wav(file):\n",
        "  return malaya_speech.load(file)[0]\n",
        "\n",
        "p = Pipeline()\n",
        "frame = p.foreach_map(load_wav).map(model)\n",
        "r = p.emit(speakers)\n",
        "\n",
        "# calculate similarity\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "1 - cdist(r['speaker-vector'], r['speaker-vector'], metric = 'cosine')"
      ],
      "metadata": {
        "id": "TrQZzxCeIEDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Transformer"
      ],
      "metadata": {
        "id": "eFSLUBfjNWw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import warnings\n",
        "from python_speech_features import mfcc, delta\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.base import BaseEstimator, TransformerMixin"
      ],
      "metadata": {
        "id": "D8CFmG5ENP-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self, winlen=0.020,preemph=0.95,numcep=20,nfft=1024,ceplifter=15,highfreq=6000,nfilt=55,appendEnergy=False):\n",
        "      self.winlen = winlen\n",
        "      self.preemph = preemph\n",
        "      self.numcep = numcep\n",
        "      self.nfft = nfft\n",
        "      self.ceplifter = ceplifter\n",
        "      self.highfreq = highfreq\n",
        "      self.nfilt = nfilt\n",
        "      self.appendEnergy = appendEnergy\n",
        "\n",
        "  def transform(self, x):\n",
        "      \"\"\" A reference implementation of a transform function.\n",
        "              Parameters\n",
        "              ----------\n",
        "              x : {array-like, sparse-matrix}, shape (n_samples, n_features)\n",
        "                  The input samples.\n",
        "              Returns\n",
        "              -------\n",
        "              X_transformed : array, shape (n_samples, n_features)\n",
        "                  The array containing the element-wise square roots of the values\n",
        "                  in ``X``.\n",
        "              \"\"\"\n",
        "      # Check is fit has been called\n",
        "      check_is_fitted(self, 'n_features_')\n",
        "\n",
        "      # Check that the input is of the same shape as the one passed\n",
        "      # during fit.\n",
        "      if x.shape[1] != self.n_features_:\n",
        "          raise ValueError('Shape of input is different from what was seen'\n",
        "                           'in `fit`')\n",
        "      return self.signal_to_mfcc(x)\n",
        "\n",
        "  def fit(self, x, y=None):\n",
        "      \"\"\"A reference implementation of a fitting function for a transformer.\n",
        "              Parameters\n",
        "              ----------\n",
        "              x : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
        "                  The training input samples.\n",
        "              y : None\n",
        "                  There is no need of a target in a transformer, yet the pipeline API\n",
        "                  requires this parameter.\n",
        "              Returns\n",
        "              -------\n",
        "              self : object\n",
        "                  Returns self.\n",
        "              \"\"\"\n",
        "      self.n_features_ = x.shape[1]\n",
        "      return self"
      ],
      "metadata": {
        "id": "8IGzEXreNbLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom classifier class"
      ],
      "metadata": {
        "id": "6RzI0O6BNhas"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h1uBBkIXNlpK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}