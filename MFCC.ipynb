{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/n66sm4gBNifpqhjnJk2q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inachenyx/SpeechSNN/blob/main/MFCC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing - Feature scaling (MFCC feature extraction)\n",
        "#### sklearn.preprocessing"
      ],
      "metadata": {
        "id": "hC-0IleHn6JY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8nG3WVklwG4"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "def extract_features(audio,rate):\n",
        "   mfcc_feature = mfcc.mfcc(audio,rate, winlen=0.020,preemph=0.95,numcep=20,nfft=1024,ceplifter=15,highfreq=6000,nfilt=55,appendEnergy=False)\n",
        "   mfcc_feature = preprocessing.scale(mfcc_feature)\n",
        "   delta = calculate_delta(mfcc_feature)\n",
        "   combined = np.hstack((mfcc_feature,delta))\n",
        "   return combined"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing - Stereo to mono format\n",
        "#### pydub"
      ],
      "metadata": {
        "id": "kbNzjM54oEfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "mysound = AudioSegment.from_wav(\"stereo_infile.wav\")\n",
        "# set mono channel\n",
        "mysound = mysound.set_channels(1)\n",
        "# save the result\n",
        "mysound.export(\"mono_outfile.wav\", format=\"wav\")"
      ],
      "metadata": {
        "id": "L9SAKUd_n3xj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VAD Voice Activity Detection\n",
        "#### PyTorch"
      ],
      "metadata": {
        "id": "PyQOOimWvFR8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# loading vad model and tools to work with audio\n",
        "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False)\n",
        "\n",
        "(get_speech_ts_adaptive, save_audio, read_audio, VADIterator, collect_chunks) = utils\n",
        "\n",
        "audio = read_audio('raw_voice.wav')\n",
        "\n",
        "\n",
        "# get time chunks with voice\n",
        "speech_timestamps = get_speech_ts_adaptive(wav, model)\n",
        "\n",
        "\n",
        "# gather the chunks and save them to a file\n",
        "save_audio('only_speech.wav',\n",
        "         collect_chunks(speech_timestamps, audio))"
      ],
      "metadata": {
        "id": "ONPS-XFYxKhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Noise Reduction\n",
        "### noisereduce, SciPy"
      ],
      "metadata": {
        "id": "vyicKsi32Gjy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import noisereduce as nr\n",
        "from scipy.io import wavfile\n",
        "\n",
        "# load data\n",
        "rate, data = wavfile.read(\"voice_with_noise.wav\")\n",
        "\n",
        "# perform noise reduction\n",
        "reduced_noise = nr.reduce_noise(y=data, sr=rate)"
      ],
      "metadata": {
        "id": "ygkCoW1b2DNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Extraction\n",
        "#### Ex. Delta MFCC combined with regular MFCC, numpy, scikit-learn, python_speech_features"
      ],
      "metadata": {
        "id": "Crlu9kSA57GB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from python_speech_features import mfcc, delta\n",
        "\n",
        "def extract_features(audio,rate):\n",
        "  \"\"\"extract 20 dim mfcc features from audio file, perform CMS and combine\n",
        "  delta to make 40 dim feature vector\"\"\"\n",
        "\n",
        "  mfcc_feature = mfcc.mfcc(audio, rate, winlen=0.020,preemph=0.95,numcep=20,nfft=1024,ceplifter=15,highfreq=6000,nfilt=55,appendEnergy=False)\n",
        "\n",
        "  # feature scaling\n",
        "  mfcc_feature = preprocessing.scale(mfcc_feature)\n",
        "  delta_feature = delta(mfcc_feature, 2) # calculating delta\n",
        "  # stacking delta features with common features\n",
        "  combined_features = np.hstack((mfcc_feature, delta_feature))\n",
        "  return combined_features"
      ],
      "metadata": {
        "id": "UAs7CRL16Re6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train with Gaussian Mixture Model\n",
        "#### sklearn.mixture"
      ],
      "metadata": {
        "id": "Z2lhNxkb_HBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_rate, data = read('denoised_vad_voice.wav')\n",
        "\n",
        "# extract 40 dimensional MFCC & delta MFCC features\n",
        "features = extract_features(audio, sr)\n",
        "\n",
        "gmm = GMM(n_components=16,max_iter=200,covariance_type='diag',n_init=1, init_params='random')\n",
        "gmm.fit(features)  # gmm training"
      ],
      "metadata": {
        "id": "7dtrMCnT_Otw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}