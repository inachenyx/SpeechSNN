{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inachenyx/SpeechSNN/blob/main/SpeakerExample.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSnb17zgxoJQ",
        "outputId": "d8c0365a-6aee-497d-c38e-36b22313c317"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: snntorch in /usr/local/lib/python3.11/dist-packages (0.9.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchaudio) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchaudio snntorch tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2USe_HEE69W-"
      },
      "source": [
        "### Speaker Recognition Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDT6XPUT645s"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import snntorch as snn\n",
        "from snntorch import utils\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgM5bxxnLAc3"
      },
      "source": [
        "## When not enough RAM in colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWHe5ydfLMBH"
      },
      "source": [
        "### Setup Dataset with On-the-Fly Rate Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CL0jdlCGK9-b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchaudio.datasets import SPEECHCOMMANDS\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define MFCC transform\n",
        "mfcc_transform = T.MFCC(\n",
        "    sample_rate=16000,\n",
        "    n_mfcc=13,\n",
        "    melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 40},\n",
        ")\n",
        "\n",
        "# Load SpeechCommands dataset subset\n",
        "class SubsetSC(SPEECHCOMMANDS):\n",
        "    def __init__(self, subset: str = None):\n",
        "        super().__init__(\".\", download=True)\n",
        "\n",
        "        def load_list(filename):\n",
        "            filepath = os.path.join(self._path, filename)\n",
        "            with open(filepath) as f:\n",
        "                return [os.path.join(self._path, line.strip()) for line in f]\n",
        "\n",
        "        if subset == \"validation\":\n",
        "            self._walker = load_list(\"validation_list.txt\")\n",
        "        elif subset == \"testing\":\n",
        "            self._walker = load_list(\"testing_list.txt\")\n",
        "        elif subset == \"training\":\n",
        "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
        "            excludes = set(excludes)\n",
        "            self._walker = [w for w in self._walker if w not in excludes]\n",
        "\n",
        "train_raw = SubsetSC(\"training\")\n",
        "test_raw = SubsetSC(\"testing\")\n",
        "\n",
        "# Build speaker index\n",
        "def build_speaker_index(dataset):\n",
        "    speaker_ids = sorted({data[3] for data in dataset})\n",
        "    return {sid: i for i, sid in enumerate(speaker_ids)}\n",
        "\n",
        "speaker_dict = build_speaker_index(train_raw)\n",
        "num_classes = len(speaker_dict)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c52HveWgLQra"
      },
      "source": [
        "### Custom Dataset with On-the-Fly Rate Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_IO0YCCLWPz"
      },
      "outputs": [],
      "source": [
        "class RateEncodedMFCCDataset(Dataset):\n",
        "    def __init__(self, dataset, speaker_to_idx, transform, num_steps=20, max_len=80):\n",
        "        self.dataset = dataset\n",
        "        self.speaker_to_idx = speaker_to_idx\n",
        "        self.transform = transform\n",
        "        self.num_steps = num_steps\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        waveform, sample_rate, _, speaker_id, _ = self.dataset[idx]\n",
        "        mfcc = self.transform(waveform).squeeze(0)  # [13, time]\n",
        "        mfcc = mfcc[:, :self.max_len]\n",
        "        if mfcc.shape[1] < self.max_len:\n",
        "            pad = torch.zeros(mfcc.shape[0], self.max_len - mfcc.shape[1])\n",
        "            mfcc = torch.cat([mfcc, pad], dim=1)\n",
        "\n",
        "        mfcc = (mfcc - mfcc.min()) / (mfcc.max() - mfcc.min() + 1e-5)\n",
        "        spikes = (torch.rand(self.num_steps, *mfcc.shape) < mfcc.unsqueeze(0)).float()\n",
        "\n",
        "        label = self.speaker_to_idx[speaker_id]\n",
        "        return spikes, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lZMrV5uLkxW"
      },
      "source": [
        "### LSTM-based Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqLCMdj8Ln-3"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SpikingLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):  # x: [B, T, 13, 80]\n",
        "        x = x.mean(dim=3)  # average over time axis → shape: [B, T, 13]\n",
        "        out, _ = self.lstm(x)\n",
        "        return self.fc(out[:, -1, :])  # take last output step\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhf7USVIMVf-"
      },
      "source": [
        "### Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcyrjV2_ewnS"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, optimizer, loss_fn):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for xb, yb in loader:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        output = model(xb)\n",
        "        loss = loss_fn(output, yb)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            pred = model(xb).argmax(1)\n",
        "            correct += (pred == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "    return correct / total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "nifTWR5Jezgj",
        "outputId": "26791d76-c14d-4782-97aa-15a784d2a8ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss = 7.2427, Test Acc = 0.0224\n",
            "Epoch 2: Loss = 6.5945, Test Acc = 0.0151\n",
            "Epoch 3: Loss = 6.3827, Test Acc = 0.0303\n",
            "Epoch 4: Loss = 6.0944, Test Acc = 0.0192\n",
            "Epoch 5: Loss = 5.8730, Test Acc = 0.0370\n",
            "Epoch 6: Loss = 5.7337, Test Acc = 0.0434\n",
            "Epoch 7: Loss = 5.6463, Test Acc = 0.0362\n"
          ]
        }
      ],
      "source": [
        "# DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    RateEncodedMFCCDataset(train_raw, speaker_dict, mfcc_transform),\n",
        "    batch_size=64, shuffle=True, num_workers=2, pin_memory=True\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    RateEncodedMFCCDataset(test_raw, speaker_dict, mfcc_transform),\n",
        "    batch_size=64, shuffle=False, num_workers=2, pin_memory=True\n",
        ")\n",
        "\n",
        "# Model setup\n",
        "model = SpikingLSTM(input_size=13, hidden_size=128, output_size=num_classes).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train\n",
        "for epoch in range(10):\n",
        "    loss = train_epoch(model, train_loader, optimizer, loss_fn)\n",
        "    acc = evaluate(model, test_loader)\n",
        "    print(f\"Epoch {epoch+1}: Loss = {loss:.4f}, Test Acc = {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0lyiC_iK3k2"
      },
      "source": [
        "## When have enough RAM storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrmwtAXV9A_5"
      },
      "source": [
        "### Dataset Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6RzqDtA9Hh2",
        "outputId": "7aca25c6-e5b4-4154-efd1-1d1a987e78fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2.26G/2.26G [00:32<00:00, 75.1MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Custom subclass to extract speaker labels\n",
        "class SubsetSC(SPEECHCOMMANDS):\n",
        "    def __init__(self, subset: str = None):\n",
        "        super().__init__(\".\", download=True)\n",
        "\n",
        "        def load_list(filename):\n",
        "            filepath = os.path.join(self._path, filename)\n",
        "            with open(filepath) as f:\n",
        "                return [os.path.join(self._path, line.strip()) for line in f]\n",
        "\n",
        "        if subset == \"validation\":\n",
        "            self._walker = load_list(\"validation_list.txt\")\n",
        "        elif subset == \"testing\":\n",
        "            self._walker = load_list(\"testing_list.txt\")\n",
        "        elif subset == \"training\":\n",
        "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
        "            excludes = set(excludes)\n",
        "            self._walker = [w for w in self._walker if w not in excludes]\n",
        "\n",
        "train_dataset = SubsetSC(\"training\")\n",
        "test_dataset = SubsetSC(\"testing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9St5MB_w_PnF"
      },
      "source": [
        "### Preprocessing: MFCC Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOT0x2XO_Gd4",
        "outputId": "bfd43104-3dc6-4449-a12e-538968f6c0e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 105829/105829 [11:39<00:00, 151.34it/s]\n",
            "100%|██████████| 11005/11005 [01:18<00:00, 139.89it/s]\n"
          ]
        }
      ],
      "source": [
        "mfcc_transform = T.MFCC(\n",
        "    sample_rate=16000,\n",
        "    n_mfcc=13,\n",
        "    melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 40},\n",
        ")\n",
        "\n",
        "def extract_mfcc(dataset, max_len=80):\n",
        "    features, labels, speakers = [], [], []\n",
        "    speaker_to_idx = {}\n",
        "    speaker_count = 0\n",
        "\n",
        "    for waveform, sample_rate, _, speaker_id, _ in tqdm(dataset):\n",
        "        mfcc = mfcc_transform(waveform).squeeze(0)  # [n_mfcc, time]\n",
        "        mfcc = mfcc[:, :max_len]  # crop or pad\n",
        "        if mfcc.shape[1] < max_len:\n",
        "            pad = torch.zeros(mfcc.shape[0], max_len - mfcc.shape[1])\n",
        "            mfcc = torch.cat([mfcc, pad], dim=1)\n",
        "\n",
        "        if speaker_id not in speaker_to_idx:\n",
        "            speaker_to_idx[speaker_id] = speaker_count\n",
        "            speaker_count += 1\n",
        "\n",
        "        features.append(mfcc)\n",
        "        labels.append(speaker_to_idx[speaker_id])\n",
        "\n",
        "    return torch.stack(features), torch.tensor(labels), speaker_to_idx\n",
        "\n",
        "X_train, y_train, speaker_dict = extract_mfcc(train_dataset)\n",
        "X_test, y_test, _ = extract_mfcc(test_dataset)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwLWis9e_6wT"
      },
      "source": [
        "### Spiking Input Encoding (Rate Coding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        },
        "id": "NWlbSsdR_6Fg",
        "outputId": "db04e062-bb89-404f-a73f-c6cf31a96f07"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'X_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-16a794c31e17>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnum_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mX_train_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mX_test_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencode_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Encode MFCC features as spikes: simple rate coding\n",
        "def rate_encode(x, num_steps=20):\n",
        "    x = (x - x.min()) / (x.max() - x.min())  # normalize\n",
        "    spikes = torch.rand((num_steps, *x.shape)) < x.unsqueeze(0)\n",
        "    return spikes.float()\n",
        "\n",
        "# Encode entire dataset\n",
        "def encode_dataset(X, num_steps=20):\n",
        "    return torch.stack([rate_encode(x, num_steps) for x in tqdm(X)])\n",
        "\n",
        "num_steps = 20\n",
        "X_train_encoded = encode_dataset(X_train, num_steps)\n",
        "X_test_encoded = encode_dataset(X_test, num_steps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QmEvFlUBR5P"
      },
      "source": [
        "### LSTM-Based Spiking Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uToS2jeBRO8"
      },
      "outputs": [],
      "source": [
        "class SpikingLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.readout = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, spike_input):  # [T, B, C, F]\n",
        "        T, B, C, F = spike_input.shape\n",
        "        x = spike_input.permute(1, 0, 3, 2)  # [B, T, F, C]\n",
        "        x = x.mean(-1)  # [B, T, F]\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.readout(out[:, -1, :])  # last time step\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RiBQrKWBtxE"
      },
      "source": [
        "### Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHasXpxFBpkR"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, criterion, X, y, batch_size=64):\n",
        "    model.train()\n",
        "    perm = torch.randperm(X.size(0))\n",
        "    X, y = X[perm], y[perm]\n",
        "\n",
        "    for i in range(0, len(X), batch_size):\n",
        "        xb = X[i:i+batch_size].to(device)\n",
        "        yb = y[i:i+batch_size].to(device)\n",
        "\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "def evaluate(model, X, y, batch_size=64):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(X), batch_size):\n",
        "            xb = X[i:i+batch_size].to(device)\n",
        "            yb = y[i:i+batch_size].to(device)\n",
        "            out = model(xb)\n",
        "            pred = torch.argmax(out, dim=1)\n",
        "            correct += (pred == yb).sum().item()\n",
        "            total += yb.size(0)\n",
        "\n",
        "    return correct / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAE68x3aCmIt"
      },
      "outputs": [],
      "source": [
        "input_size = X_train.shape[1]  # MFCCs = 13\n",
        "hidden_size = 128\n",
        "output_size = len(speaker_dict)\n",
        "\n",
        "model = SpikingLSTM(input_size, hidden_size, output_size).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    train(model, optimizer, criterion, X_train_encoded, y_train)\n",
        "    acc = evaluate(model, X_test_encoded, y_test)\n",
        "    print(f\"Epoch {epoch+1}: Test Accuracy = {acc:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNPYhW8ic6KgOObR23KUtwD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}